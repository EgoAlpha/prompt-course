## Transformers as Algorithms: Generalization and Stability in In-context Learning



## 介绍

​		[\[ Li et al. (2023)\]](https://arxiv.org/abs/2301.07067)从统计学的角度将ICL形式化为一个算法学习问题，其中Transformer在推理时隐式地构造假设函数，给出了MTL(Multitask Learning)的泛化边界，描述了Transformer/attention何时可证明地服从稳定性条件，并提供了经验验证，最后数学证明Transformer确实可以在具有i.i.d.(输入，输出)和动态数据的经典回归问题上实现接近最优的算法。



