*Zero-shot prompting*

*Large Language Models are Zero-Shot Reasoners*

## 介绍

[\[小岛康誉等人，2022年\]](https://arxiv.org/abs/2205.11916)通过在提示中的每个答案前简单地添加“让我们一步一步地思考”,显示LLM是体面的零样本推理器。实验结果表明，使用相同的单提示模板，这种所谓的零镜头CoT在包括算术和其他逻辑推理任务在内的各种基准推理任务上显著优于零镜头LLM性能，而无需任何手工制作的少镜头示例。

## 它是如何工作的？

大型语言模型(LLM)的成功通常归因于(在上下文中)少量或零量学习。它可以通过简单地将模型限制在几个例子(少量镜头)或描述任务的指令(零镜头)上来解决各种任务。对语言模型进行条件化的方法称为“提示”，无论是手动还是自动设计提示都已经成为NLP中的热门话题。虽然CoT提示以及许多其他特定任务提示工作的成功通常归功于LLM的少量学习能力，但我们通过添加一个简单的提示“让我们一步一步地思考”，以促进在回答每个问题之前一步一步地思考，表明LLM是体面的零次推理者。尽管简单，但零投CoT成功地以零投方式生成了一条似是而非的推理路径，并在标准零投方法失败的问题中找到了正确答案。

重要的是，Zero-shot-CoT是通用的和任务不可知的，不同于大多数先前的以示例(少量)或模板(Zero-shot)形式的特定于任务的提示工程:它可以促进跨各种推理任务的逐步回答，包括算术、符号推理、常识推理和其他逻辑推理任务，而无需修改每个任务的提示。见下图。

![](../images/Zero_shot_CoT_prompting.png)

提示示例：

*提示:*

*问:一个变戏法的人可以变戏法16个球。一半球是高尔夫球，一半高尔夫球是蓝色的。有多少蓝色的高尔夫球？*

*a:让我们一步一步来考虑。*

*输出:*

*总共有16个球。一半的球是高尔夫球。这意味着有8个高尔夫球。一半的高尔夫球是蓝色的。这意味着有4个蓝色的高尔夫球。*
