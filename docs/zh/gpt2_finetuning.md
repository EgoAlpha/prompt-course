# ğŸ•¯ï¸ GPT2-finetuningå†…éƒ¨æ„é€ 

## 1.1 tokenåŒ–

å¯¹æ•°æ®é›†è¿›è¡Œå¤„ç†éƒ¨åˆ†æ˜¯åœ¨encode.pyå†…è¿›è¡Œæ“ä½œï¼š

``` python
def main():
    args = parser.parse_args()
    enc = encoder.get_encoder(args.model_name, models_dir=args.models_dir)
    print('Reading files')
    chunks = load_dataset(enc, args.in_text, args.combine, encoding=args.encoding)
    print('Writing', args.out_npz)
    np.savez_compressed(args.out_npz, *chunks)

```

NLPé¢„è®­ç»ƒæ¨¡å‹å¤§éƒ½é€šè¿‡embedding æ¥è¡¨ç¤ºè¯ä¹‰ï¼Œgptä¸­é€šè¿‡BPE(byte pair encoding)æ–¹æ³•æ¥è¿›è¡Œåˆ†è¯ã€‚

ä»¥ä¸‹æ˜¯gpt2-finetuningä¸­encoder.pyå†…bpeçš„å®ç°æ–¹æ³•

``` python
# å°†è¯è¡¨å¾ä¸ºunicodeä»£ç 
def bytes_to_unicode():
    """
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
    This is a signficant percentage of your normal, say, 32K bpe vocab.
    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
    And avoids mapping to whitespace/control characters the bpe code barfs on.
    """
    bs = list(range(ord("!"), ord("~")+1))+list(range(ord("Â¡"), ord("Â¬")+1))+list(range(ord("Â®"), ord("Ã¿")+1))
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8+n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))
```

``` python
#bpeä¸­å°†æ¯ä¸ªè¯è¿›è¡Œåˆ†è§£
#ä¾‹å¦‚å°†ï¼ˆ"hello"ï¼‰åˆ†è§£æˆ("h""e","e""l",""l"l"...)
def get_pairs(word):
    """Return set of symbol pairs in a word.

    Word is represented as tuple of symbols (symbols being variable-length strings).
    """
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs

```

``` python
#å¼€å§‹è®­ç»ƒå‰ä¼šè°ƒç”¨encoder
class Encoder:
    def __init__(self, encoder, bpe_merges, errors='replace'):
        self.encoder = encoder
        self.decoder = {v:k for k,v in self.encoder.items()}
        self.errors = errors # how to handle errors in decoding
        self.byte_encoder = bytes_to_unicode()
        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}
        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))  #bpe_mergesä¸­è¡¨ç¤ºä¸ºæ¯ç§è¯çš„å¸¸ç”¨åº¦æ’å
        self.cache = {}

        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")#ä¸€äº›ç‰¹æ®Šå­—ç¬¦çš„æ“ä½œï¼Œåœ¨åé¢ä¼šç”¨åˆ°

```

``` python
#å¦‚æœè¯åœ¨å†…éƒ¨ç¼“å­˜ä¸­åˆ™ç›´æ¥è¿”å›ã€‚
def bpe(self, token):
       if token in self.cache:
           return self.cache[token]
       word = tuple(token)
       #è¿›è¡Œgetpairsæ“ä½œ
       pairs = get_pairs(word)
       #è¯åˆ†ä¸äº†pairåˆ™è¿”å›å®Œæ•´çš„è¯ï¼Œè¯å¾ˆçŸ­çš„æƒ…å†µä¸‹
       if not pairs:
         return token
       #å¯¹pairè¯å¯¹è¿›è¡Œæ“ä½œ
       while True:
          # å°†è¾“å…¥çš„pairs æŒ‰ç…§.bpeæ–‡ä»¶ ï¼ˆå¸¸ç”¨æ’åï¼‰æ’åº è¿™é‡Œçš„pair å°±æ˜¯ 55è¡Œæåˆ° a b
          # æ‰¾åˆ°æœ€å¸¸ç”¨çš„å“ªä¸ª pair float('inf') è¡¨ç¤ºæ— ç©·å¤§ æ‰¾ä¸åˆ°çš„è¯ å°±è¿”å›æ— é™å¤§çš„å€¼ ä»¥å…è¢«é€‰ä¸Š
          bigram = min(pairs, key = lambda pair:
          #MIN MAX ä¸­key ç›¸å½“äºä¾æ®ä»€ä¹ˆæ’åº
          self.bpe_ranks.get(pair, float('inf')))
           # ç»„åˆä¸åœ¨bpeè¡¨æ ¼ä¸­ pairsä¸­ä¸èƒ½å†æ‹†äº† å¾ªç¯ç»“æŸ
           if bigram not in self.bpe_ranks:
               break
            # æ‹¿åˆ°ç¬¬ä¸€ä¸ªè¯ ç¬¬äºŒä¸ªè¯   
           first, second = bigram
           new_word = []
           i = 0
            #  æŸ¥æ‰¾å­ä¸²
           while i < len(word):
               try:
                   j = word.index(first, i)# iæŒ‡çš„æ˜¯ä»ç¬¬Iä¸ªå¼€å§‹æŸ¥æ‰¾  #æŸ¥æ‰¾list.index(x,èµ·å§‹ä½ç½®,ç»ˆæ­¢ä½ç½®) #ä»ä¼ å…¥çš„wordé‡Œ æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå•è¯
                    # è¿™é‡Œçš„æ„æ€æ˜¯ å› ä¸ºpair æ˜¯æ— åºçš„ è¦æ‰¾åˆ°å…¶åœ¨è¾“å…¥è¯ä¸­çš„é¡ºåº
                   new_word.extend(word[i:j])# å°†è¿™ä¸ªå­ä¸² first=word[i:j] æ”¾å…¥new_wordå˜é‡ä¸­
                   i = j
               except:
                   new_word.extend(word[i:])# å½“Jè¶Šç•Œæ—¶å€™ ç›´æ¥å°† i: åˆ‡ç‰‡æ”¾è¿›å»
                   break
               # è¿™é‡Œçš„æ„æ€æ˜¯ å¦‚æœfirst å’Œ second è¿™ä¸¤ä¸ªæ˜¯è¿ç€çš„è¯ åŠ åˆ°new_wordæ—¶å€™ æ˜¯ä¸€ä¸ªæ•´ä½“
               if word[i] == first and i < len(word)-1 and word[i+1] == second:
                   new_word.append(first+second)
                   i += 2
               else:
                   new_word.append(word[i])
                   i += 1
           #é€’å½’æ“ä½œ
           new_word = tuple(new_word)
           word = new_word
           if len(word) == 1:
               break
           else:
               pairs = get_pairs(word)
       word = ' '.join(word)
       #å°†è¯æ”¾å…¥ç¼“å­˜ä¸­
       self.cache[token] = word
       return word
```

``` python
# ä¸åœ¨self.encoderè¯å…¸çš„è¯ ç¼–ç è¿‡ç¨‹
def encode(self, text):
    bpe_tokens = []
    #self.pat .findall text çš„æ„æ€æ˜¯ä»text ä¸­ æŠŠself.patè¿™é‡Œpatternæ‰¾å‡ºæ¥ å…¶å®å°±æ˜¯she's å˜æˆ she sä¸¤ä¸ªå•è¯
    for token in re.findall(self.pat, text):
        token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))
        #ä¸Šé¢ä¸€å¥å¤§è‡´ç­‰ä»·äº token = unicode(token, "utf-8") #å°†æ–‡å­—è½¬æˆutf-8å ç”¨self.byte_encoderâ€”â€”bytes_to_unicode()äº§ç”Ÿçš„dict è½¬å›å­—ç¬¦å½¢å¼ ç„¶åå°†å…¶è¿åŸå­—ç¬¦ä¸²
        bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))
         #å°†æ‹†å®Œçš„è¯ åœ¨ä¼ å…¥çš„embeddingå­—å…¸ä¸­æŸ¥æ‰¾ï¼Œè¿”å›è¿™ä¸ªåˆ—è¡¨
    return bpe_tokens

```

ä»¥ä¸Šæ˜¯encoder.pyå†…å®¹ï¼Œencoder.pyæ˜¯ä¸ºäº†åœ¨encodeä¸­å°†è¯è¿›è¡Œbpeå¤„ç†çš„æ–¹æ³•ã€‚ç°åœ¨è¿”å›encode.pyä¸­çš„æ­¤è¡Œ

~~~python
chunks = load_dataset(enc, args.in_text, args.combine, encoding=args.encoding)
~~~

æ­¤å¤„åº”ç”¨äº†load_dataset.pyåŠŸèƒ½ï¼Œ
[](https://medium.com/analytics-vidhya/understanding-the-gpt-2-source-code-part-2-4a980c36c68b)

``` python
def load_dataset(enc, path, combine, encoding=None):
    paths = []
    if os.path.isfile(path):
        # Simple file
        paths.append(path)
    elif os.path.isdir(path):
        # Directory
        for (dirpath, _, fnames) in os.walk(path):
            for fname in fnames:
                paths.append(os.path.join(dirpath, fname))
    else:
        # Assume glob
        paths = glob.glob(path)

    token_chunks = []
    raw_text = ''
    for path in tqdm.tqdm(paths):
        if path.endswith('.npz'):
            # Pre-encoded
            with np.load(path) as npz:
                for item in npz.files:
                    token_chunks.append(npz[item])
        else:
            # Plain text
            with open(path, 'r', encoding=encoding) as fp:
                raw_text += fp.read()
            if len(raw_text) >= combine:
                tokens = np.stack(enc.encode(raw_text))
                token_chunks.append(tokens)
                raw_text = ''
            else:
                raw_text += '<|endoftext|>'
    if raw_text:
        tokens = np.stack(enc.encode(raw_text))
        token_chunks.append(tokens)
    return token_chunks
```

## 1.2 modelå‡½æ•°ã€

é¦–å…ˆæ˜¯modelçš„ä¸»ä½“modelå‡½æ•°å¦‚ä¸‹ï¼š

``` python
#Xå’Œpastçš„è¾“å…¥æƒ…å†µï¼š

#Xæ˜¯è¯­è¨€æ¨¡å‹è¾“å…¥ï¼Œpastæ˜¯å·²ç”Ÿæˆä¸Šæ–‡çš„çŠ¶æ€ã€‚
#è®­ç»ƒæ—¶ï¼ŒXä¸ºä¸€ç»„è®­ç»ƒæ•°æ®[2]ï¼Œpastä¸ºç©ºã€‚
#æ¡ä»¶ç”Ÿæˆåˆå§‹é˜¶æ®µï¼ŒXä¸ºæ¡ä»¶è¯­å¥ï¼Œpastä¸ºç©º
#æ— æ¡ä»¶ç”Ÿæˆåˆå§‹é˜¶æ®µï¼ŒXä¸º[end]ï¼Œpastä¸ºç©º
#ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼ŒXä¸ºä¸Šä¸€æ¬¡ç”Ÿæˆçš„è¯è¯­ï¼Œpastä¸ºä¹‹å‰æ‰€æœ‰Kå’ŒVã€‚
def model(hparams, X, past=None, scope='model', reuse=tf.AUTO_REUSE):
    with tf.variable_scope(scope, reuse=reuse):
        results = {}
        batch, sequence = shape_list(X)

        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],
                             initializer=tf.random_normal_initializer(stddev=0.01))
        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],
                             initializer=tf.random_normal_initializer(stddev=0.02)) #vte vpe è¯å‘é‡çŸ©é˜µ wteè¯è¯­åµŒå…¥çŸ©é˜µ wpeä½ç½®åµŒå…¥çŸ©é˜µ
        past_length = 0 if past is None else tf.shape(past)[-2]  #å·²ç”Ÿæˆä¸Šæ–‡çš„é•¿åº¦
        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))  #ç”±è¾“å…¥xæä¾›çš„ä¿¡æ¯ vteä¸vpeåŠ å’Œè€Œæ¥ shapeä¸º[æ‰¹æ¬¡å¤§å°ï¼Œè¾“å…¥é•¿åº¦ï¼ŒåµŒå…¥ç»´åº¦]


        # Transformer
        presents = []
        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer
        assert len(pasts) == hparams.n_layer
        for layer, past in enumerate(pasts):
            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)   #hæœ€ç»ˆçš„shapeæ˜¯[batch, seq, embd]
            if layer == 10:
                tf.add_to_collection('checkpoints', h)
            presents.append(present)
        results['present'] = tf.stack(presents, axis=1)
        h = norm(h, 'ln_f')            #å°†hå±•å¹³æˆ[batch*seq,embd],ç”¨çŸ©é˜µä¹˜æ³•ä¹˜ä»¥word embdè½¬æ¢çŸ©é˜µï¼ˆshape=[vocab,embd]ï¼‰ï¼Œæœ€åå†reshapeï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºlogits

        # Language model loss.  Do tokens <n predict token n?
        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])

        #
        logits = tf.matmul(h_flat, wte, transpose_b=True)
        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])
        results['logits'] = logits
        return results

```

é¦–å…ˆçœ‹ç¬¬ä¸€éƒ¨åˆ†

>``` python
>results = {}
>batch, sequence = shape_list(X)
>
>wpe = tf.get_variable('wpe', [hparams.n_ctx, >hparams.n_embd],
>                     >initializer=tf.random_normal_initializer(stddev=0.01))
>wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],
>                     initializer=tf.random_normal_initializer(stddev=0.02)) #vte vpe è¯å‘é‡çŸ©é˜µ wteè¯è¯­åµŒå…¥çŸ©é˜µ wpeä½ç½®åµŒå…¥çŸ©é˜µ
>past_length = 0 if past is None else tf.shape(past)[-2]  #å·²ç”Ÿæˆä¸Šæ–‡çš„é•¿åº¦
>h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))
>```

é¦–å…ˆå°†æ¯ä¸ªè¯­å¥åˆ†è§£æˆä¸¤ä¸ªæƒé‡çŸ©é˜µï¼Œwpeå’Œwteï¼Œä¹Ÿå°±æ˜¯å•è¯çš„è¯åµŒå…¥å‘é‡å’Œå•è¯ä½ç½®ç¼–ç ã€‚
![](https://img-blog.csdnimg.cn/f743f4d544c040799cb76063181bea28.png#pic_center)
![](https://img-blog.csdnimg.cn/a09bb1aeb1be4db996d73e17c1c3d5d9.png#pic_center)

å¤„ç†å®Œembeddingåï¼Œå°†è¿™äº›çŸ©é˜µå‘ä¸Šå‘åˆ°blockå—ä¸­ï¼Œ
>
>``` python
>
presents = []
        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer
        assert len(pasts) == hparams.n_layer
        for layer, past in enumerate(pasts):
            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)   #hæœ€ç»ˆçš„shapeæ˜¯[batch, seq, embd]
            if layer == 10:
                tf.add_to_collection('checkpoints', h)
            presents.append(present)
        results['present'] = tf.stack(presents, axis=1)
>```

ç°åœ¨æ¥çœ‹blockå—ï¼Œæ¯ä¸ªblockå¯ä»¥çœ‹åšæ˜¯æ•°æ®å¤„ç†çš„ä¸€å±‚ï¼Œå®ƒåŒ…å«äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œä»¥åŠæ®‹å·®è¿æ¥ã€‚
![](https://pic2.zhimg.com/80/v2-428430661223cad84784fee06d091ea1_720w.webp)

``` python
def block(x, scope, *, past, hparams):
    with tf.variable_scope(scope):
        nx = shape_list(x)[-1]
        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)  #å–embeddingç»´åº¦ä¸ºnxå’ŒåšAttentionã€‚æ³¨æ„è¿™é‡Œåœ¨åšAttentionå±‚ä¹‹å‰å…ˆå¯¹xåšnormalization
        x = x + a   #æ®‹å·®æ“ä½œï¼Œè¿™é‡Œçš„'+'æ˜¯element-wise plusã€‚ æ­¤æ—¶x=a=[batch,seq,embedding]ï¼ŒåŠ å’Œä¹‹åä¾ç„¶æ˜¯[batch,seq,embedding]ã€‚
        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)  #mæ˜¯ç»è¿‡mlpè¿›ä¸€æ­¥æå–çš„ç‰¹å¾ã€‚
        x = x + m #å°†mlpå¾—åˆ°çš„ä¿¡æ¯mæ®‹å·®åŠ å’Œåˆ°å·²æœ‰ä¿¡æ¯xã€‚
        return x, present     #è¿”å›æœ€ç»ˆç»“æœ + present [batch, seq, embd]
```

blockä¸­å°†å‰ä¸€å±‚çš„è¾“å‡ºä½œä¸ºè¾“å…¥é¦–å…ˆè¿›è¡Œäº†å·²çŸ¥ä¿¡æ¯xä¸æ³¨æ„åŠ›æœºåˆ¶ä¸‹çš„è¾“å‡ºaæ®‹å·®ç›¸åŠ ï¼Œå…¶ä¸­è‡ªæ³¨æ„åŠ›æ“ä½œå¦‚ä¸‹ï¼š<br>
1.ä¸ºæ¯ä¸ªå•è¯åˆ›å»ºquery,key,value å‘é‡ <br>
2.å¯¹äºæ¯ä¸ªè¾“å…¥tokenï¼Œä½¿ç”¨å…¶queryå‘é‡å¯¹å…¶ä»–æ‰€æœ‰çš„tokençš„keyå‘é‡è¿›è¡Œè¯„åˆ†ï¼Œè·å¾—æ³¨æ„åŠ›åˆ†æ•°ã€‚<br>
3.å°†valueå‘é‡ä¹˜ä»¥ä¸Šä¸€æ­¥å¾—åˆ°çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œè¿›è¡Œç›¸åŠ ã€‚<br>
å…¶ä¸­q,k,vå‘é‡åˆ†åˆ«ä»£è¡¨ç€ï¼š<br>
query:æ˜¯å½“å‰å•è¯çš„è¡¨ç¤ºå½¢å¼ï¼Œç”¨äºå¯¹æ‰€æœ‰å…¶ä»–å•è¯ï¼ˆkeyï¼‰è¿›è¡Œè¯„åˆ†ï¼Œæˆ‘ä»¬åªéœ€è¦å…³æ³¨å½“å‰æ­£åœ¨å¤„ç†çš„tokençš„queryã€‚<br>
Key:å¯ä»¥çœ‹åšæ˜¯åºåˆ—ä¸­æ‰€æœ‰å•è¯çš„æ ‡ç­¾ï¼Œæ˜¯åœ¨æˆ‘ä»¬æ‰¾ç›¸å…³å•è¯æ—¶å€™çš„å¯¹ç…§ç‰©ã€‚<br>
Value:å•è¯çš„å®é™…è¡¨ç¤ºï¼Œä¸€æ—¦æˆ‘ä»¬å¯¹æ¯ä¸ªå•è¯çš„ç›¸å…³åº¦æ‰“åˆ†ä¹‹åï¼Œæˆ‘ä»¬å°±è¦å¯¹valueè¿›è¡Œç›¸åŠ è¡¨ç¤ºå½“å‰æ­£åœ¨å¤„ç†çš„å•è¯çš„valueã€‚<br>

### 1.2.1è‡ªæ³¨æ„åŠ›æ“ä½œ

ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºquery,key,valueå‘é‡
![](https://img-blog.csdnimg.cn/ce2c13f280e6468eb4204ce9133e160e.png#pic_center)ä»£ç ä¸­å¯¹åº”æ“ä½œæ˜¯ï¼š

``` python
  c = conv1d(x, 'c_attn', n_state*3)
  q, k, v = map(split_heads, tf.split(c, 3, axis=2))
```

å·ç§¯æ–¹æ³•æ˜¯:

``` python

#è¯¥å‡½æ•°å°†xåŸæœ‰çš„nxä¸ªfeatureçº¿æ€§å˜æ¢ä¸ºnfä¸ªfeatureï¼Œå¯ä»¥çœ‹ä½œä¸€ç»´å·ç§¯ï¼Œä¹Ÿå¯ä»¥çœ‹ä½œä¸€ä¸ªLinearå±‚ã€‚
#nxæ˜¯xç›®å‰featureæ•°ï¼Œstartæ˜¯å…¶ä½™shapeã€‚shape_listå‡½æ•°æ˜¯å°†xçš„shapeä»¥åˆ—è¡¨è¿”å›ã€‚
#wæ˜¯ç¬¬ä¸€å±‚linearçš„å‚æ•°ï¼Œå®šä¹‰ä¸º[1,åŸfeatureæ•°ï¼Œæ–°featureæ•°çš„å˜é‡ï¼Œå¹¶ä»¥æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ã€‚
#bæ˜¯ç¬¬ä¸€å±‚linearçš„åç§»å€¼ï¼Œå®šä¹‰ä¸º[æ–°feature]æ•°çš„å˜é‡ï¼Œå¹¶ä»¥å¸¸å€¼0åˆå§‹åŒ–ã€‚
#cæ˜¯çº¿æ€§å˜æ¢çš„ç»“æœï¼Œè¿™å¥è¯ç­‰ä»·äºC=WX+B
#æœ€å¤–å±‚tf.reshapeçš„ç›®çš„æ˜¯å°†ç»“æœè½¬åŒ–ä¸ºæ­£ç¡®çš„ä¸‰ç»´shapeï¼Œstart+[nf]æ˜¯åˆ—è¡¨çš„æ‹¼æ¥ï¼Œç»“åˆå‰é¢startçš„å®šä¹‰ï¼Œå…¶å®å°±æ˜¯å°†æœ€åä¸€ç»´nxæ¢ä¸ºnfã€‚
#å†…å±‚å…ˆreshapeä¸ºäºŒç»´ï¼ŒåšçŸ©é˜µä¹˜ï¼ŒåŠ ä¸Šåç½®ã€‚
def conv1d(x, scope, nf, *, w_init_stdev=0.02):
    with tf.variable_scope(scope):
        *start, nx = shape_list(x)
        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))
        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))
        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])
        return c


```

ç¬¬äºŒæ­¥ï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œé€šè¿‡queryå’Œkeyå‘é‡ï¼Œå°†ç¬¬ä¸€ä¸ªtokençš„queryä¸å…¶ä»–tokençš„keyå‘é‡ç‚¹ä¹˜ï¼Œå¾—åˆ°æ¯ä¸€ä¸ªtokençš„æ³¨æ„åŠ›åˆ†æ•°
![](https://img-blog.csdnimg.cn/2d7f4819b5d14d589be7967a3c5a1f62.png#pic_center)
ç¬¬ä¸‰æ­¥ï¼šæ±‚å’Œï¼Œå¯¹äºæ¯ä¸€ä¸ªtokenå°†ä¸Šä¸€æ­¥å¾—åˆ°çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œä¸valueå‘é‡ç›¸ä¹˜ï¼Œå†ç´¯åŠ softmaxåŒ–å¾—åˆ°æœ€åçš„æ³¨æ„åŠ›åˆ†æ•°å æ¯”
![](https://img-blog.csdnimg.cn/e8cd9278eca04029926cb46be9179453.png#pic_center)
è¿™ä¸¤æ­¥çš„ä»£ç åœ¨multhead_attnæ–¹æ³•ä¸­ä½“ç°ï¼Œè¯¥ä»£ç ä¼šåœ¨ä¹‹åè¿›è¡Œè§£é‡Šã€‚

### 1.2.2 å¸¦æ©ç çš„è‡ªæ³¨æ„åŠ›æ“ä½œ

ä»¥ä¸Šæ˜¯transformerä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯gptç”¨çš„æ˜¯å¸¦maskedçš„è‡ªæ³¨æ„åŠ›ï¼Œå®ƒä¸æ™®é€šçš„è‡ªæ³¨æ„åŠ›ä¸åŒçš„æ˜¯ï¼Œæ¯ä¸ªæ­£åœ¨å¤„ç†çš„tokenï¼Œåè¾¹çš„tokençš„æ³¨æ„åŠ›å¾—åˆ†ä¸º0ï¼Œå³åªä¼šå…³æ³¨å½“å‰tokenå‰é¢çš„è¾“å…¥å¥å­![](https://img-blog.csdnimg.cn/e4ef703a5ba944019d5bd04a76de463b.png#pic_center)

ä¾‹å¦‚ï¼Œè¾“å…¥åºåˆ—â€œrobot must obey ordersâ€ä¸­ã€‚æ¯ä¸ªå•è¯ä½œä¸ºä¸€ä¸ªtokenï¼Œå¯¹äºå¤„ç†åºåˆ—æ‰¹é‡å¤§å°ä¸º4çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å°†æ•´ä¸ªåºåˆ—ä½œä¸ºä¸€ä¸ªbatchè¿›è¡Œå¤„ç†ã€‚
![](https://img-blog.csdnimg.cn/86c192d48d4c423eae6c2a672b1dda9e.png#pic_center)
æ­¤æ—¶tokenæ— æ³•ç›´æ¥è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œå› æ­¤éœ€è¦ç”¨tokenå¯¹åº”çš„queryä¸keyè¿›è¡Œè®¡ç®—
![](https://img-blog.csdnimg.cn/847e971aa41d4b71b82616b7a99eefe0.png#pic_center)
å®Œæˆä¹˜æ³•è¿ç®—åï¼ŒåŠ ä¸ŠmaskçŸ©é˜µæ¥å±è”½æ‰å½“å‰è¿˜ä¸ºè¾“å…¥çš„è¯ï¼Œé€šå¸¸æ˜¯å°†maskçŸ©é˜µè®¾ç½®æˆä¸Šä¸‰è§’çŸ©é˜µï¼Œå±è”½ä½ç½®çš„æ•°å€¼ä¸ºéå¸¸å¤§çš„è´Ÿæ•°ã€‚
![](https://img-blog.csdnimg.cn/92c9791d9cb042f99aa5de97953a8a8e.png#pic_center)
å†è¿›è¡ŒsoftmaxåŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼š
![](https://img-blog.csdnimg.cn/2116cc94748b4522838ded41e8bd01ea.png#pic_center)
ä¸Šè¡¨ä¸­å¯ä»¥ç†è§£ä¸ºï¼Œå½“åªè¾“å…¥ä¸€ä¸ªå•è¯robotæ—¶ï¼Œrobotçš„æ³¨æ„åŠ›å¾—åˆ†ä¸º1ï¼Œè¾“å…¥ç¬¬äºŒä¸ªè¯mustæ—¶ï¼Œrobotæ³¨æ„åŠ›å¾—åˆ†ä¸º0.48,mustå¾—åˆ†ä¸º0.52...å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªè¯çš„åè¾¹çš„è¯æ³¨æ„åŠ›å¾—åˆ†çš†ä¸º0

åœ¨ä»£ç ä¸­ï¼Œä¸Šè¿°éƒ¨åˆ†æ“ä½œåœ¨mask_attn_weightsä¸­ï¼Œè¿™éƒ¨åˆ†æ˜¯è®¾ç½®æ©ç çš„æƒé‡çŸ©é˜µã€‚

### 1.2.3 gpt2ä¸­çš„mask self attention

ç°åœ¨å‡è®¾ç”¨gpt2æ¨¡å‹æ¥è¿›è¡Œé¢„æµ‹ä»»åŠ¡ï¼Œæ¨¡å‹åœ¨æ¯æ¬¡è¿­ä»£åéƒ½ä¼šæ–°å¢ä¸€ä¸ªè¯ï¼Œæ­¤æ—¶ä¸ºäº†å¢åŠ æ¨¡å‹å¤„ç†æ•ˆç‡ï¼Œgpt2ä¸­ä¸æ˜¯ç®€å•çš„æ¯è¾“å…¥ä¸€ä¸ªtokenå°±å¯¹å‰é¢æ‰€æœ‰tokenè¿›è¡Œç‚¹ä¹˜æ³¨æ„åŠ›è®¡ç®—ï¼Œå®ƒä¼šå¯¹å‰é¢çš„token keyå’Œvalueå‘é‡è¿›è¡Œä¿å­˜ï¼Œä»è€Œåœ¨è¿­ä»£ä¸­æå–
![](https://img-blog.csdnimg.cn/b7cecaefa5d94b46be15a9b35ec4633f.png#pic_center)
![](https://img-blog.csdnimg.cn/25fd7990a4d2468482b547d772d5d110.png#pic_center)
![](https://img-blog.csdnimg.cn/388e83e2f493423aaed5970d0af92f07.png#pic_center)
è€Œgpt2ä¸­çš„tokençš„qkvå‘é‡æ˜¯é€šè¿‡è‡ªæ³¨æ„åŠ›å±‚ä¹˜ä»¥æƒå€¼çŸ©é˜µå¾—åˆ°è¿™ä¸ªtokençš„queryï¼Œkeyï¼Œvalueçš„æ‹¼æ¥å‘é‡ã€‚
![](https://img-blog.csdnimg.cn/f4057661f9be44438511b9e11c365960.png#pic_center)
![](https://img-blog.csdnimg.cn/d04f985bcd4742e0b126cecea223594f.png#pic_center)
![](https://img-blog.csdnimg.cn/866d1e922cc04bf5b8015035f2ab6e56.png#pic_center)
åœ¨é€šè¿‡åˆ’åˆ†å¤šå¤´æ¥å¾—åˆ°æ¯ä¸ªå¤´çš„valueè¯„åˆ†ã€‚
![](https://img-blog.csdnimg.cn/9c198be0586a433a95be9a7220008294.png#pic_center)
![](https://img-blog.csdnimg.cn/dd6f5d85d9a24aa79f9ceb34b5fcf3b7.png#pic_center)

åœ¨é€šè¿‡æ¯ä¸ªvalueä¸å®ƒçš„æ³¨æ„åŠ›åˆ†æ•°ç›¸ä¹˜ç›¸åŠ ï¼Œå¾—åˆ°è¿™ä¸ªå¤šå¤´çš„è‡ªæ³¨æ„åŠ›ç»“æœã€‚
![](https://img-blog.csdnimg.cn/7c9a61489de844f299dd8ec81e4daf52.png#pic_center)
![](https://img-blog.csdnimg.cn/74ee8188ceb04a00a425222fa93c89fa.png#pic_center)
![](https://img-blog.csdnimg.cn/66b275cb31e74f4bbcd9c4bcaa0af516.png#pic_center)
æœ€åé€šè¿‡åˆå¹¶å¤šå¤´å¾—åˆ°æœ€ç»ˆç»“æœã€‚
![](https://img-blog.csdnimg.cn/fece316433b744768bcd006ce042240c.png#pic_center)

``` python
def attn(x, scope, n_state, *, past, hparams):
    assert x.shape.ndims == 3  # Should be [batch, sequence, features]
    assert n_state % hparams.n_head == 0
    if past is not None:
        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]

    def split_heads(x):
        # From [batch, sequence, features] to [batch, heads, sequence, features]
        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])   #é¦–å…ˆåº”ç”¨split_statesï¼Œå°†xè½¬æ¢ä¸º[batch,seq,head,feature]ã€‚ç„¶åä½¿ç”¨tf.transposeï¼Œå°†åŸæœ‰ç»´åº¦é‡æ’ä¸º[batch,head,seq,feature]ï¼Œä¹Ÿå°±æ˜¯å°†ç¬¬ä¸€ç»´å’Œç¬¬äºŒç»´äº¤æ¢ä½ç½®ã€‚

#é¦–å…ˆå°†Aè½¬åŒ–ä¸º[batchï¼Œè¾“å…¥é•¿åº¦ï¼Œheadï¼Œfeature]ï¼Œä¾ç„¶æ˜¯é€šè¿‡tf.transposeäº¤æ¢ä¸­é—´ä¸¤ç»´å®ç°çš„ã€‚

    def merge_heads(x):
        # Reverse of split_heads
        return merge_states(tf.transpose(x, [0, 2, 1, 3]))

    def mask_attn_weights(w):
        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.  ndä¸ºè¾“å…¥é•¿åº¦ï¼Œnsä¸ºæ€»é•¿åº¦ã€‚
        _, _, nd, ns = shape_list(w)  #Gpt-2ç­‰ä¸€ç³»åˆ—Transformerç”Ÿæˆæ¨¡å‹ä½¿ç”¨masked attentionï¼Œä¸»è¦æ˜¯ä¸ºäº†é¿å…æ¨¡å‹åœ¨ç”Ÿæˆç¬¬iä¸ªè¯æ—¶ä½¿ç”¨iä¹‹åçš„è¯è¯­ï¼Œ å› ä¸ºåœ¨å®é™…é¢„æµ‹æ—¶åé¢çš„è¯æ˜¯ä¸å¯çŸ¥çš„ã€‚
        b = attention_mask(nd, ns, dtype=w.dtype) #bä¸ºé0å³1çš„maskçŸ©é˜µï¼Œåé¢ä¼šå°†bä¸wç›¸ä¹˜ã€‚
        b = tf.reshape(b, [1, 1, nd, ns])   #å°†è¿”å›çš„maskçŸ©é˜µreshapeä¸ºå››ç»´[11]ï¼Œç„¶åä¸æƒé‡çŸ©é˜µåšelement-wiseçš„ä¹˜æ³•ã€‚åé¢å‡å»1e10*(1-b)ï¼Œå½“bä¸º1æ—¶æ— æ•ˆæœï¼Œå½“bä¸º0æ—¶ï¼Œç­‰äºå‡å»1e10ï¼Œä¸€ä¸ªå¾ˆå¤§çš„å€¼ï¼Œå¯¼è‡´å€¼å˜ä¸º-10eï¼Œä¹Ÿå°±å¯¼è‡´softmaxä¹‹åæƒé‡å˜ä¸º0.
        w = w*b - tf.cast(1e10, w.dtype)*(1-b)
        return w  #è¯¥å‡½æ•°è¿”å›maskä¹‹åçš„æƒé‡çŸ©é˜µWï¼Œå‡†ç¡®åœ°è¯´ï¼Œæ˜¯å°†æƒé‡çŸ©é˜µç¬¬iè¡Œçš„ä¸å¯attendåˆ—ç½®ä¸º0ã€‚

    def multihead_attn(q, k, v):
        # q, k, v have shape [batch, heads, sequence, features]
        w = tf.matmul(q, k, transpose_b=True)   #matmulä¸­transpose_bå‚æ•°æ„å‘³ç€ä¹˜ä¹‹å‰å°†Kè½¬ç½®ã€‚
        w = w * tf.rsqrt(tf.cast(shape_list(v)[-1], w.dtype)) #æ³¨æ„Q=[batch,head,è¾“å…¥é•¿åº¦ï¼Œfeature]ï¼ŒK=[batch,head,æ€»é•¿åº¦,feature]ï¼Œmatmulå¯¹æœ€åäºŒç»´è¿›è¡Œï¼Œå…¶å®ä¹Ÿå°±æ˜¯Qçš„featureå’ŒKçš„featureåšç‚¹ç§¯ï¼ŒW=[batchï¼Œheadï¼Œè¾“å…¥é•¿åº¦ï¼Œæ€»é•¿åº¦]ï¼Œè¡¨ç¤ºVçš„å¾—åˆ†ã€‚

        w = mask_attn_weights(w)  #å‚è€ƒä¸Šé¢maskattnweight
        w = softmax(w)  #å°†æƒé‡çŸ©é˜µåšä¸€æ¬¡softmaxï¼Œå°†æƒé‡å½’ä¸€ä¸º[0,1]ä¹‹é—´ä¸”å’Œä¸º1ã€‚
        a = tf.matmul(w, v)  #æ­¤æ—¶W=[batch,head,è¾“å…¥é•¿åº¦ï¼Œæ€»é•¿åº¦]ï¼ŒV=[batch,head,æ€»é•¿åº¦ï¼Œfeature]å¾—åˆ°A=[batch,head,è¾“å…¥é•¿åº¦ï¼Œfeature]ï¼Œè¿™å°±æ˜¯Attentionæœºåˆ¶æ‰€æå–çš„ç‰¹å¾ã€‚
        return a

    with tf.variable_scope(scope):
        c = conv1d(x, 'c_attn', n_state*3)  #å°†xé€šè¿‡ä¸€æ¬¡ä¸€ç»´å·ç§¯ï¼Œä»embeddingä¸­æå–n_state*3ä¸ªç‰¹å¾ï¼Œgpt-2ä¸­n_state=embeddingã€‚æ­¤æ—¶c=[batch,seq,embedding*3]
        q, k, v = map(split_heads, tf.split(c, 3, axis=2))#ä½¿ç”¨tf.splitå°†ç‰¹å¾åˆ†ç»™qï¼Œkï¼Œvã€‚æ­¤æ—¶Q=K=V=[batch,seq,embedding]
        present = tf.stack([k, v], axis=1)  #presentæ˜¯tf.stackå®Œæˆçš„kå’Œvçš„å †å ï¼Œè¿™ä¸€é¡¹ä¼šä½œä¸ºè¿”å›å€¼è¿”å›ï¼Œå¹¶ä¸”ä¸ä¹‹å‰çš„çŠ¶æ€æ‹¼æ¥ï¼Œä½œä¸ºself-attentionçš„å¯¹è±¡ã€‚
        if past is not None:
            pk, pv = tf.unstack(past, axis=1)
            k = tf.concat([pk, k], axis=-2)
            v = tf.concat([pv, v], axis=-2) #è¿™ä¸€æ®µå°±æ˜¯ä»ä¹‹å‰çš„çŠ¶æ€åˆ†å‡ºkå’Œvï¼Œå°†å…¶æ‹¼æ¥åˆ°å½“å‰çš„kå’Œvä¸Šã€‚æ‹¼æ¥ä¹‹å‰k=v=[batch,head,å½“å‰é•¿åº¦,feature]ï¼Œåœ¨æ‹¼æ¥ä¹‹åk=v=[batch,head,å·²ç”Ÿæˆé•¿åº¦+å½“å‰é•¿åº¦,feature]
        a = multihead_attn(q, k, v)  #aæ˜¯attnå±‚çš„è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯ä¹‹åçš„hï¼Œæ˜¯Qå’ŒKå¯¹çš„VåŠ æƒå’Œ
        a = merge_heads(a)  #åˆå¹¶å¤šå¤´ï¼Œè¿™æ˜¯åˆ†è§£å¤šå¤´çš„é€†è¿‡ç¨‹ã€‚
        a = conv1d(a, 'c_proj', n_state)  #æœ€åçš„çº¿æ€§å˜æ¢ã€‚
        return a, present
```

åœ¨æ³¨æ„åŠ›æ“ä½œä¸­,ä¼šå¯¹è¾“å…¥å¼ é‡è¿›è¡Œä»¥ä¸‹å‡ ä¸ªæ“ä½œï¼š
é¦–å…ˆå¯¹è¯å‘é‡è¿›è¡Œæ©ç æ“ä½œï¼š

``` python
def attention_mask(nd, ns, *, dtype):

    i = tf.range(nd)[:,None]
    j = tf.range(ns)
    m = i >= j - ns + nd
    return tf.cast(m, dtype)
```

åœ¨åˆ†è§£å¤šå¤´ä»¥åŠåˆå¹¶æ—¶ï¼Œéœ€è¦ç”¨åˆ°split_states,ä»¥åŠmerge_stateså‡½æ•°ï¼š

``` python

#è¯¥ç¨‹åºå°†xçš„æœ€åä¸€ç»´åˆ†è§£ä¸ºäºŒç»´ï¼Œå³åˆ†å‡ºå¤šå¤´ç»´

def split_states(x, n):
    """Reshape the last dimension of x into [n, x.shape[-1]/n]."""
    *start, m = shape_list(x)
    return tf.reshape(x, start + [n, m//n])
```

``` python
#å°†æœ€åä¸¤ç»´reshapeä¸ºä¸€ç»´ï¼Œä¹Ÿå°±æ˜¯å°†headä¸ªfeatureé¡ºåºå †å ã€‚
#æ­¤æ—¶A=[batchï¼Œè¾“å…¥é•¿åº¦ï¼Œhead*feature=embedding]ï¼Œ

def merge_states(x):
    """Smash the last two dimensions of x into a single dimension."""
    *start, a, b = shape_list(x)
    return tf.reshape(x, start + [a*b])
```

åœ¨æ“ä½œå®Œæ³¨æ„åŠ›æœºåˆ¶åï¼Œç„¶åå¯¹æ–°ç»“æœæå–ç‰¹å¾åœ¨é™„ç€åˆ°xä¸­ï¼Œå…¨è¿æ¥ç¥ç»ç½‘ç»œçš„è¾“å…¥æ˜¯è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡ºï¼Œç”¨äºå¤„ç†è‡ªæ³¨æ„åŠ›å­å±‚å¾—åˆ°çš„tokençš„æ–°çš„è¡¨ç¤ºï¼Œè¿™ä¸ªæ–°çš„è¡¨ç¤ºåŒ…å«äº†åŸå§‹tokenåŠå…¶ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ã€‚
![](https://img-blog.csdnimg.cn/53208c6c1c5141bcba5928aa0b6c6353.gif#pic_center)
ç¬¬ä¸€å±‚å°†å‘é‡è½¬æ¢æˆæ¨¡å‹å¤§å°çš„å¤šè¢«
![](https://img-blog.csdnimg.cn/aa0137914fa543e7a101293ca6577219.gif#pic_center)
ç¬¬äºŒå±‚å°†ç¬¬ä¸€å±‚çš„ç»“æœå†æŠ•å°„å›æ¨¡å‹çš„ç»´åº¦ã€‚
mlpçš„æ“ä½œå¦‚ä¸‹:

``` python
def mlp(x, scope, n_state, *, hparams):  #n_stateè¡¨ç¤ºç¬¬ä¸€å±‚çº¿æ€§å˜æ¢çš„ç‰¹å¾ç»´åº¦ã€‚
    with tf.variable_scope(scope):
        nx = shape_list(x)[-1]
        h = gelu(conv1d(x, 'c_fc', n_state))
        h2 = conv1d(h, 'c_proj', nx)  #çº¿æ€§å˜æ¢åˆ°n_stateç»´ï¼Œgeluæ¿€æ´»ï¼Œå†å˜æ¢å›nxç»´ã€‚
        return h2
```

ä»¥ä¸Šæ˜¯blockéƒ¨åˆ†ï¼Œæˆ‘ä»¬ç»§ç»­çœ‹modeléƒ¨åˆ†ï¼Œåœ¨å°†è¯å‘é‡è¾“å…¥åˆ°blockä¸­å¹¶æå–ç»“æœåï¼Œä¼šè¿›è¡Œå¦‚ä¸‹æ“ä½œ:

``` python
        presents = []
        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer
        assert len(pasts) == hparams.n_layer
        for layer, past in enumerate(pasts):
            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)   #hæœ€ç»ˆçš„shapeæ˜¯[batch, seq, embd]
            if layer == 10:
                tf.add_to_collection('checkpoints', h)
            presents.append(present)
        results['present'] = tf.stack(presents, axis=1)
        h = norm(h, 'ln_f')            #å°†hå±•å¹³æˆ[batch*seq,embd],ç”¨çŸ©é˜µä¹˜æ³•ä¹˜ä»¥word embdè½¬æ¢çŸ©é˜µï¼ˆshape=[vocab,embd]ï¼‰ï¼Œæœ€åå†reshapeï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºlogits

        # Language model loss.  Do tokens <n predict token n?
        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])

        #
        logits = tf.matmul(h_flat, wte, transpose_b=True)
        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])
        results['logits'] = logits
        return results
```

è¿™éƒ¨åˆ†çš„æ“ä½œæ˜¯å°†ç»“æœä»ä¸‰ç»´çš„è¯å‘é‡å±•å¹³æˆäºŒç»´ï¼Œæ–¹ä¾¿åç»­è¿›è¡Œå­—å…¸æŸ¥è¯¢ä»¥åŠæ¦‚ç‡è¡¨ç¤ºã€‚

## 1.3 å¼€å§‹è®­ç»ƒ

å¤„ç†å®Œtokenåï¼Œå°†æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œé¦–å…ˆæ˜¯å„ç§å‚æ•°è®¾ç½®ï¼šä¾‹å¦‚batchæ•°é‡batch_size,å­¦ä¹ ç‡learning_rate,ä¼˜åŒ–å™¨optimizerç­‰ï¼Œä»¥åŠæ¨¡å‹é€‰æ‹©ï¼Œæ•°æ®é›†é€‰æ‹©ç­‰ã€‚è¿™äº›å‚æ•°å‡åœ¨parserä¸­è¿›è¡Œä¿®æ”¹ã€‚

``` python
def main():
    args = parser.parse_args()
    enc = encoder.get_encoder(args.model_name, models_dir=args.models_dir)
    hparams = model.default_hparams() ## è¯»å–é»˜è®¤å‚æ•°
    with open(os.path.join('models', args.model_name, 'hparams.json')) as f:  ## é¢„è®­ç»ƒä¸­çš„æ¨¡å‹å‚æ•°
        hparams.override_from_dict(json.load(f))  ## å‚æ•°é‡å†™

    if args.sample_length > hparams.n_ctx:    ## è¿™é‡Œè¦æ±‚æˆ‘ä»¬è®¾ç½®çš„ä¸€ä¸ªå¥å­çš„é•¿åº¦ä¸èƒ½å¤§äºé¢„è®­ç»ƒæ¨¡å‹çš„
        raise ValueError(
            "Can't get samples longer than window size: %s" % hparams.n_ctx)
```

ä¹‹åæ˜¯è®­ç»ƒé›†éªŒè¯é›†æ„å»ºï¼š

``` python
with tf.Session() as sess:
    # Fully static shape required to make memory accounting in
    # twremat accurate.
    train_context = tf.placeholder(tf.int32, [args.batch_size, 1024])  ## å ä½
    train_context_in = randomize(train_context, hparams, args.noise)   ## è®¾ç½®ä¸ºè¾“å…¥
    train_output = model.model(hparams=hparams, X=train_context_in)   ### è°ƒç”¨gpt-2çš„model
    train_loss = tf.reduce_mean(
        tf.nn.sparse_softmax_cross_entropy_with_logits(
            labels=train_context[:, 1:], logits=train_output['logits'][:, :-1]))

    if args.val_every > 0:   ## éªŒè¯æ•°æ®æ„å»º
        val_context = tf.placeholder(tf.int32, [args.val_batch_size, None])
        val_output = model.model(hparams=hparams, X=val_context)
        val_loss = tf.reduce_mean(       #ç§»è½´å¹³å‡å€¼
            tf.nn.sparse_softmax_cross_entropy_with_logits(   #è®¡ç®—è¯¯å·®  æŸå¤±å‡½æ•°
                labels=val_context[:, 1:], logits=val_output['logits'][:, :-1]))
        val_loss_summary = tf.summary.scalar('val_loss', val_loss)
```

å…¶ä¸­ä¼šåœ¨è®­ç»ƒé›†ä¸­æ·»åŠ éšæœºå™ªéŸ³ï¼š

``` python
def randomize(context, hparams, p):  ## éšæœºmaskã€æ·»åŠ noise
    if p > 0:
        mask = tf.random.uniform(shape=tf.shape(context)) < p
        noise = tf.random.uniform(shape=tf.shape(context), minval=0, maxval=hparams.n_vocab, dtype=tf.int32)
        return tf.where(mask, noise, context)
    else:
        return context

```

æ„å»ºå®ŒéªŒè¯é›†åè¿›è¡Œéœ€è¦è®­ç»ƒæ›´æ–°çš„å‚æ•°ï¼š

``` python
all_vars = [v for v in tf.trainable_variables() if 'model' in v.name]   ## è·å¾—æ‰€æœ‰è¦æ›´æ–°çš„å‚æ•°ï¼›  tf.trainable_variables () æŒ‡çš„æ˜¯éœ€è¦è®­ç»ƒçš„å˜é‡
      train_vars = [v for v in all_vars if '/h' in v.name] if args.only_train_transformer_layers else all_vars   ## ä»…ä»…è®­ç»ƒ/hé‡Œçš„å‚æ•°

```

é€‰æ‹©ä¼˜åŒ–å™¨ä»¥åŠç®—æ³•ï¼š

``` python
if args.optimizer == 'adam':
    print('Using Adam optimizer', file=sys.stderr)
    opt = tf.train.AdamOptimizer(learning_rate=args.learning_rate)
elif args.optimizer == 'sgd':
    print('Using SGD optimizer', file=sys.stderr)
    opt = tf.train.GradientDescentOptimizer(learning_rate=args.learning_rate)
else:
    exit('Bad optimizer:', args.optimizer)

if args.memory_saving_gradients:
    if tf.VERSION >= '2':
        exit('Memory saving gradients are not supported in tensorflow 2.x')
    import memory_saving_gradients
    opt_grads = memory_saving_gradients.gradients(train_loss, train_vars)  ## é€šè¿‡train_loss å¯¹train_varsæ±‚æ¢¯åº¦
elif args.twremat:
    import tfremat
    opt_grads = tf.gradients(train_loss, train_vars)
    (train_loss, opt_grads) = tfremat.tf_remat((train_loss, opt_grads), memlimit=args.twremat_memlimit)
else:
    opt_grads = tf.gradients(train_loss, train_vars)
```

è¿›è¡Œè®­ç»ƒï¼š

``` python
opt_grads = list(zip(opt_grads, train_vars))
       opt_apply = opt.apply_gradients(opt_grads)  ## è¿›è¡Œæ¢¯åº¦ä¸‹é™
       summary_loss = tf.summary.scalar('loss', train_loss)  # ç”¨æ¥æ˜¾ç¤ºæ ‡é‡ä¿¡æ¯
       summary_lr = tf.summary.scalar('learning_rate', args.learning_rate)
       summaries = tf.summary.merge([summary_lr, summary_loss])

       summary_log = tf.summary.FileWriter(
           os.path.join(CHECKPOINT_DIR, args.run_name))
       #ä¿å­˜æ¨¡å‹
       saver = tf.train.Saver(
           var_list=all_vars,
           max_to_keep=5,
           keep_checkpoint_every_n_hours=2)
       sess.run(tf.global_variables_initializer()) ## åˆå§‹åŒ–å˜é‡
```

ä¿å­˜ç‚¹å’Œå¯¼å…¥é¢„è®­ç»ƒçš„æ¨¡å‹ï¼š

``` python
if args.restore_from == 'latest':
           ckpt = tf.train.latest_checkpoint(
               os.path.join(CHECKPOINT_DIR, args.run_name))
           if ckpt is None:
               # Get fresh GPT weights if new run.
               ckpt = tf.train.latest_checkpoint(
                   os.path.join('models', args.model_name))
       elif args.restore_from == 'fresh':
           ckpt = tf.train.latest_checkpoint(
               os.path.join('models', args.model_name))  ## å¯¼å…¥é¢„è®­ç»ƒæ¨¡å‹
       else:
           ckpt = tf.train.latest_checkpoint(args.restore_from)
       print('Loading checkpoint', ckpt)
       saver.restore(sess, ckpt)  ## æ¨¡å‹æ¢å¤ saver.restore(sess,æ•°æ®è·¯å¾„)
```

## 1.4 æ ·ä¾‹ç”Ÿæˆ

åœ¨gpt2-fintuningä¸­ è¿›è¡Œè®­ç»ƒæ—¶ä¼šè¿›è¡Œæ ·ä¾‹ç”Ÿæˆï¼Œæ ·ä¾‹ç”Ÿæˆä»£ç åœ¨sample.pyä¸­

å€™é€‰tokens å¯ä»¥æ ¹æ®temperatureå‚æ•°å¤§å°æ¥æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§å’Œåˆ›é€ åŠ›ã€‚æ¸©åº¦è¶Šé«˜ï¼Œç”Ÿæˆçš„æ–‡æœ¬è¶Šå¤šæ ·åŒ–ï¼Œä½†ä¹Ÿå¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬ä¸å¤ªå‡†ç¡®æˆ–ä¸è¿è´¯ã€‚æ¸©åº¦è¶Šä½ï¼Œç”Ÿæˆçš„æ–‡æœ¬åˆ™è¶Šæ¥è¿‘è®­ç»ƒæ•°æ®ï¼Œä½†ä¹Ÿå¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬è¿‡äºä¿å®ˆå’Œé‡å¤
![](https://pic4.zhimg.com/80/v2-c59319cddb6ae9e44cd970b69f970a97_720w.webp)

å­—è¯çš„é€‰æ‹©æ˜¯æ ¹æ®top_p ä»¥åŠ top_k è¿›è¡Œé€‰å–ï¼Œtop_pä»¥åŠtop_k å‡æ˜¯ç”¨æ¥é˜²æ­¢ç”Ÿæˆç»“æœè¿›å…¥å¾ªç¯
å…¶ä¸­top_k å‚æ•°ç”¨æ¥é€‰å–å€™é€‰tokensä¸­æ¦‚ç‡å‰kä¸ªtokensåšä¸ºä¸‹ä¸€æ­¥å€™é€‰tokens

``` python
def top_k_logits(logits, k): ## è®¡ç®—top_k
    if k == 0:
        # no truncation
        return logits

    def _top_k():
        values, _ = tf.nn.top_k(logits, k=k)
        min_values = values[:, -1, tf.newaxis]
        return tf.where(
            logits < min_values,
            tf.ones_like(logits, dtype=logits.dtype) * -1e10,
            logits,
        )
    return tf.cond(
       tf.equal(k, 0),
       lambda: logits,
       lambda: _top_k(),
    )

```

top_pæ–¹æ³• ç”¨æ¥é€‰å–å€™é€‰tokensä¸­æ¦‚ç‡ç´¯åŠ è¾¾åˆ°pé˜ˆå€¼çš„å‰å‡ ä¸ªtokensåšä¸ºä¸‹ä¸€æ­¥å€™é€‰tokens

``` python
ef top_p_logits(logits, p):  ### è®¡ç®—top_p, ç­‰äº1æ—¶ç›¸å½“äºæ²¡è®¡ç®—
    with tf.variable_scope('top_p_logits'):
        logits_sort = tf.sort(logits, direction='DESCENDING')
        probs_sort = tf.nn.softmax(logits_sort)
        probs_sums = tf.cumsum(probs_sort, axis=1, exclusive=True)
        logits_masked = tf.where(probs_sums < p, logits_sort, tf.ones_like(logits_sort)*1000) # [batchsize, vocab]
        min_logits = tf.reduce_min(logits_masked, axis=1, keepdims=True) # [batchsize, 1]  # æŒ‰ç…§indicesçš„æ ¼å¼ä»sorted_logitsä¸­æŠ½å–åˆ‡ç‰‡
        return tf.where(  # è‹¥condition=True,åˆ™è¿”å›å¯¹åº”Xçš„å€¼ï¼ŒFalseåˆ™è¿”å›å¯¹åº”çš„Yå€¼ã€‚
            logits < min_logits,
            tf.ones_like(logits, dtype=logits.dtype) * -1e10,
            logits,
        )

```

å†äº†è§£å®Œè¾“å‡ºå‚æ•°åï¼Œæ¥çœ‹gpt2-finetuningçš„æ ·ä¾‹è¾“å‡ºè¿‡ç¨‹ï¼š

``` python
def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, top_p=0.0):
    if start_token is None:
        assert context is not None, 'Specify exactly one of start_token and context!'
    else:
        assert context is None, 'Specify exactly one of start_token and context!'
        context = tf.fill([batch_size, 1], start_token)   #value=1  fill() èµ·å§‹tokenä¸ºå¼€å¤´

    def step(hparams, tokens, past=None):
        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)

        logits = lm_output['logits'][:, :, :hparams.n_vocab]
        presents = lm_output['present']
        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))
        return {
            'logits': logits,
            'presents': presents,
        }

    with tf.name_scope('sample_sequence'):
        def body(past, prev, output):
            next_outputs = step(hparams, prev, past=past)    # shape=(1, ?, 50257)
            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)    ## åªè¦æœ€åä¸€ä¸ªè¾“å‡ºçš„å€¼ï¼ˆå¯èƒ½å€¼çš„æ¦‚ç‡å‘é‡ï¼‰
            if top_p > 0.0:
                logits = top_p_logits(logits, p=top_p)
            else:
                logits = top_k_logits(logits, k=top_k)   ## [00,00,0.2,00,1,] æ¦‚ç‡
            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)  #logitsæ˜¯ä¸€ä¸ªäºŒç»´å¼ é‡ï¼Œnum_samplesæŒ‡çš„æ˜¯é‡‡æ ·çš„ä¸ªæ•°ã€‚å…¶å®å¾ˆå¥½ç†è§£ï¼Œæˆ‘ä»¬ç”Ÿæˆæ¯ä¸ªæ—¶åˆ»çš„ logits æ—¶ï¼Œè¾“å‡ºç»´åº¦åº”è¯¥æ˜¯ [ batch_size, vocab_size ] å½¢å¼çš„ï¼Œä»£è¡¨ç€è¯¥æ—¶åˆ»ï¼Œæ¯ä¸€ä¸ªbatchå¯¹åº”çš„è¯å…¸ä¸­å„è¯æ±‡ç”Ÿæˆçš„æ¦‚ç‡ã€‚tf.multinomial() å°†æŒ‰ç…§è¯¥æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼Œè¿”å›çš„å€¼æ˜¯ logits ç¬¬äºŒç»´ä¸Šçš„ idï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬éœ€è¦çš„å­—å…¸çš„ idã€‚

            return [
                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),  # present æ˜¯æ¯ä¸€å±‚çš„[k,v]
                samples,
                tf.concat([output, samples], axis=1)
            ]

        past, prev, output = body(None, context, context)

        def cond(*args):
            return True

        _, _, tokens = tf.while_loop(     # å¾ªç¯  loop_varsæ—¢æ˜¯è¾“å‡ºå€¼ä¹Ÿæ˜¯ä¸‹æ¬¡å¾ªç¯çš„è¾“å…¥å€¼
            cond=cond, body=body,
            maximum_iterations=length - 1,
            loop_vars=[
                past,
                prev,
                output
            ],
            shape_invariants=[
                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),
                tf.TensorShape([batch_size, None]),
                tf.TensorShape([batch_size, None]),
            ],
            back_prop=False,
        )

        return tokens
```

## 1.5 tokené€‰å–å¯è§†åŒ–ç¿»è¯‘

ä»¥ä¸‹å†…å®¹æè¿°å¦‚ä½•å°†å¯è§†åŒ–éšè—å±‚å¹¶æ‰¾åˆ°gpt2é€‰å–tokençš„é€»è¾‘
>ç¿»è¯‘è‡ªï¼šFinding the Words to Say: Hiddent State Visualizations for Language Models <https://jalammar.github.io/hidden-states/> <br>
ä»£ç å®ç°ï¼š<https://colab.research.google.com/github/jalammar/ecco/blob/main/>

å¯è§†åŒ–æ•ˆæœ:å½“æ¨¡å‹ç”Ÿæˆå¥å­æ—¶ï¼Œå°†æ¯ä¸€ä¸ªè¾“å‡ºçš„è¯åœ¨è¯¥å±‚ä¸­çš„åˆ†æ•°æ’åºåˆ—å‡ºï¼Œå¹¶ç”¨é¢œè‰²æ·±æµ…æ¥ä»£è¡¨å®ƒçš„åˆ†æ•°æ’è¡Œå¤§å°.
![](https://jalammar.github.io/images/explaining/rankings-gpt2xl.png )

### 1.5.1 Scores after each layer

ä¸‹å›¾æ¼”ç¤ºäº†åŸºäºtransfromerçš„è¯­è¨€æ¨¡å‹æ˜¯å¦‚ä½•é€šè¿‡å±‚ä¹‹é—´è¿ç®—æ¥å¾—åˆ°éšè—æ€ï¼Œä»¥åŠæœ€ç»ˆçš„tokenæ˜¯å¦‚ä½•æ˜ å°„åˆ°æ­¤è¡¨ä¸­å¹¶ä¸”å¯¹å…¶ä»–å¯èƒ½å¾—åˆ°çš„tokenè¿›è¡Œæ ‡è®°åˆ†æ•°ã€‚ä¾‹å¦‚å½“è¾“å…¥"1,1,"æ—¶ï¼Œä¸‹ä¸€ä¸ªè¯æ¨¡å‹59%ç¡®å®šä¸º"1"ï¼Œä»¥åŠ18%çš„æ¦‚ç‡åˆ¤å®šæ˜¯"2"(å¯èƒ½æˆ‘ä»¬åœ¨æ­£å‘è®¡æ•°)
![](https://jalammar.github.io/images/explaining/transformer-language-model-steps.png)

è®ºæ–‡æ‰€ç”¨çš„å¼€æºä»£ç Ecco æä¾›äº†æ¨¡å‹å¾—åˆ†æœ€é«˜çš„tokenä»¥åŠå…¶ä»–å€™é€‰tokenåŠå…¶æ¦‚ç‡åˆ†æ•°ã€‚

``` python
# Generate one token to complete this input string
output = lm.generate(" 1, 1, 1,", generate=1)

# Visualize
output.layer_predictions(position=6, layer=5)
```

![](https://jalammar.github.io/images/explaining/prediction_scores.PNG)

å¦å¤–ï¼Œé™¤äº†å¯¹äºæœ€ç»ˆç»“æœå±‚çš„æ¦‚ç‡åˆ†æ•°è¡¨ç¤ºï¼Œæ¯ä¸€å±‚çš„è¾“å‡ºéƒ½ä¼šè¿›è¡Œæ¦‚ç‡åˆ†æ•°è¡¨ç¤º
![](https://jalammar.github.io/images/explaining/predictions.PNG)

å°†ä»¥ä¸Šçš„æ¯å±‚è¾“å‡ºæ¦‚ç‡åˆ†æ•°è¡¨ç¤ºè¿›è¡Œåˆå¹¶ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªtokenæ¦‚ç‡åˆ†æ•°çŸ©é˜µï¼Œå¦‚ä¸‹ï¼š(æ¯è¡Œé€šè¿‡å¯¹éšè—çŠ¶æ€æŠ•å½±åˆ°è¯æ±‡è¡¨æ¥è·å–å¯¹åº”çš„è¾“å‡ºè¯ï¼Œå¹¶å°†å®ƒçš„logitsåˆ†æ•°softmaxåŒ–å¾—åˆ°æ¦‚ç‡åˆ†æ•°ï¼Œè¿™ä¸ªæ ·ä¾‹ä¸­ï¼Œç¬¬0å±‚å‰åæ²¡æœ‰é¢„æµ‹æ•°å­—ï¼Œç¬¬ä¸€å±‚"1"ä¹Ÿä»…ä»…æ˜¯0.03çš„æ¦‚ç‡ï¼Œä½†æ˜¯åé¢"1"å¾—åˆ°äº†100%çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæœ€åä¸€å±‚é€šè¿‡å‰èŠ‚æåˆ°çš„å‚æ•°å°†æ¦‚ç‡è¿›è¡Œå‘æ•£ï¼Œæœ€ç»ˆå®šæ ¼åˆ°äº†59%çš„æ¦‚ç‡åˆ†æ•°)
![](https://jalammar.github.io/images/explaining/predictions%20all%20layers.PNG)

### 1.5.2 Evolution of the selected token

å¦ä¸€ç§å¯è§†åŒ–æ˜¯å°†æœ€ç»ˆç»“æœçš„(ä¹Ÿå°±æ˜¯"1")åœ¨æ¯å±‚çš„æ’åè¿›è¡Œæ±‡æ€»å¦‚ä¸‹ï¼š
![](https://jalammar.github.io/images/explaining/logit_ranking_1.png)
å¯ä»¥çœ‹åˆ°ç¬¬0å±‚ä¸­"1"åœ¨ç¬¬31ä½ï¼Œè€Œä»ç¬¬ä¸‰å±‚å¼€å§‹æ’åä¸€ç›´åœ¨ç¬¬ä¸€ä½

ä¸‹é¢å¯¹ç¬¬äºŒç§æ–¹æ³•è¿›è¡Œæ‹“å±•ï¼Œè®©å®ƒæ¥è¡¨ç¤ºä¹‹åçš„è¾“å‡ºçš„ç´¯åŠ çŠ¶æ€ï¼Œä¾‹å¦‚è¾“å…¥"1,1,1," æˆ‘ä»¬ç°åœ¨æ¥è¡¨ç¤ºè¾“å‡º"1,1"åœ¨æ¯å±‚çš„æ’åå¦‚å›¾ï¼š
![](https://jalammar.github.io/images/explaining/sequence_111_rankings.PNG)

ä¸‹é¢å¯¹é€šå¸¸çš„å¸¦promptå¥å­è¿›è¡Œæ¼”ç¤ºï¼Œ
å½“æˆ‘ä»¬è¾“å…¥ï¼š
>"The country of the European Union are : \n "<br>
"1 . Austria \n"<br>
"2 . Belgium \n"<br>
"3 . Bulgaria \n"<br>
"4 . "

é€šè¿‡å¯è§†åŒ–éšè—å±‚å¯ä»¥çœ‹åˆ°æ¯ä¸ªå±‚æ˜¯å¦‚ä½•é€‰å–è¯å’Œå¯¹äºæœ€ç»ˆè¾“å‡ºç»“æœçš„å½±å“ï¼š
![](https://jalammar.github.io/images/explaining/ranking-eu-gpt2.png)

å¯ä»¥çœ‹å‡ºå¯¹äºæ¢è¡Œç¬¦ä»¥åŠå¥ç‚¹ï¼Œæ¨¡å‹å¾ˆæ—©å°±å·²ç»å¯¹æ­¤è¿›è¡Œæ ‡è®°ï¼Œè€Œä¸”ä¹‹åæ²¡æœ‰å¼‚è®®ã€‚æ¨¡å‹åœ¨ç¬¬ä¹å±‚åï¼Œå¯¹æ•°å­—"5" "6"è¿›è¡Œäº†æ­£ç¡®çš„é€’å¢é¢„æµ‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼šæ¨¡å‹é”™è¯¯çš„å°†æ™ºåˆ©åˆ—å…¥åˆ°æ¬§ç›Ÿå›½å®¶ä¸­ï¼Œä½†æ˜¯é”™è¯¯çš„å¹¶ä¸æ˜¯æ¨¡å‹æœ¬èº«ï¼Œå› ä¸º"chile"åœ¨æ¨¡å‹é¢„æµ‹åˆ—è¡¨ä¸­æ’åç¬¬43ï¼Œæ­¤æ—¶åº”è¯¥æ£€æŸ¥æˆ‘ä»¬ä¸ŠèŠ‚æåˆ°çš„é‡‡æ ·å‚æ•°æ¥å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œä¿®æ”¹ã€‚(å¦å¤–ï¼Œé™¤äº†é¢„æµ‹å›½å®¶å¤–ï¼Œæ¨¡å‹ä¹Ÿæ­£ç¡®çš„å¯¹å›½å®¶è¿›è¡Œäº†é¦–å­—æ¯æ’åº)

### 1.5.3 Evolution of the selected token

é™¤äº†å¯¹è¾“å‡ºtokençš„æ¦‚ç‡æ’åæ¼”å˜ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå¾—åˆ°å…¶ä»–tokençš„æ¦‚ç‡æ¼”å˜ï¼Œä¾‹å¦‚åœ¨å¯¹äºæ¨¡å‹åˆ†æå•å¤æ•°å’Œä¸»å¥å¯¹è±¡æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è¾“å…¥ï¼š
>"The only acceptable answers are 1) is 2) are"<br>
"The keys to the cabinet __"

æ¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦çŸ¥é“æˆ‘ä»¬è¦è¡¨è¾¾çš„ä¸»è¦æ˜¯keysè¿˜æ˜¯cabinetï¼Œç¡®å®šä¸»è¯­åï¼Œå†æ¥ç¡®å®šä½¿ç”¨"is" è¿˜æ˜¯ "are" ä¸‹å›¾ä¸­ï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹å¯¹äºisè¿˜æ˜¯areçš„æ¯å±‚æ¦‚ç‡æ’å
![](https://jalammar.github.io/images/explaining/watch_keys_cabinet.png)
åŒæ ·çš„æˆ‘ä»¬å¯ä»¥å°†é¢˜ç›®æ”¹æˆï¼š
>"The key to the cabinets__"

å¾—åˆ°æ¯å±‚çš„æ¦‚ç‡æ’åå›¾ï¼š![](https://jalammar.github.io/images/explaining/watch_key_cabinets.png)

é€šè¿‡ä¸Šè¿°æ–¹æ³•å¯ä»¥æ¢ç©¶æ¨¡å‹å¯¹äºåè§ç°è±¡çš„äº§ç”Ÿæƒ…å†µï¼Œä¾‹å¦‚å¯¹äºä¸åŒèŒä¸šçš„æ€§åˆ«æœŸæœ›
![](https://jalammar.github.io/images/explaining/doctor.png)
