# Introduction

Prompt engineering involves designing, creating, and optimizing prompts for the purpose of extracting accurate, consistent, and fair outputs from large language models (LLMs).

> The aforementioned is also applicable to other large generative models, such text-to-image synthesizers. For the sake of brevity, we will temporarily focus on language models.

LLMs are trained on vast amounts of text data, allowing them to encode a substantial amount of factual information about the world. Their popularity has surged in recent years due to their ability to generate human-like text, making them ideal for chatbots, virtual assistants, and similar applications. However, it is essential to note that without appropriate prompt engineering, the generated outputs can be unpredictable and potentially result in harmful consequences.

The goal of prompt engineering is to identify an appropriate prompt to be given to LLMs, in such a way that they generate responses that can effectively solve our specified complex tasks.

A prompt can take on any form of sentence such as statements, instructions, questions, or even paragraph, as long as it inspires the imagination of LLMs and guides them to explore a variety of topics and tasks.

> For the sake of making it easier for beginners to understand, we will temporarily set aside the concept of soft prompts.**

After receiving various types of prompts, LLMs have the ability to generate a diverse range of output formats, including completed sentences, question-answers, translations, conversational scripts, and other text generation, all of which have wide-ranging applications and great creative potential.

Depending on the knowledge and experience acquired from vast amounts of language data during training, LLMs are not just used for chatbot conversations, but also highly effective problem solvers.

### ðŸ’¥ Hightlight: To enhance the problem-solving ability of LLMs, high-quality prompts are crucial.

