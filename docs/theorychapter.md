# 👓 Theory Chapter
This chapter is divided into six sections, namely the overview of large language models, Transformer, tokenization, BERT model, fine-tuning of GPT model, and T5 model. It mainly introduces the basic framework structure, code implementation, and specific usage methods.
## 📔 Contents

### 👉[LLM Overview]()
本节对大模型的概述从以下几个方面展开：
- 模型介绍：首先要介绍大模型的基本信息以及它的应用领域和意义。

- 模型结构：介绍大模型的结构，包括层级结构和连接方式。通过字描述、图表或者公式等形式展示模型的结构，帮助读者理解模型的组成部分和各个部分之间的关系。

- 模型训练：介绍大模型的训练方法和数据集。详细了描述训练过程中所使用的算法、超参数的选择、训练数据的准备等。同时介绍了训练过程中遇到的挑战和解决方法，以及训练结果的评估指标。

- 模型性能：大模型在各个任务上的性能表现。本节列举出一些任务或者基准数据集，并展示大模型在这些任务上的表现，比如准确率、召回率等指标。并和其他模型进行对比，展示大模型的优势和不足之处。

- 局限性与未来工作：大模型的局限性和可能的改进方向。针对大模型在一些特定任务或者场景下的表现不足进行说明，并提出一些改进的思路和方向。与此同时，展望了未来可能的研究方向，以及大模型在其他领域的应用潜力。

### 👉[Transformer](./zh/Transformer_md/Transformer.md)
本节从以下几个方面展示具体内容：

- 本节介绍了Transformer模型的背景和意义，以及它在自然语言处理等领域的应用。
- 提出传统序列模型（如循环神经网络）存在的问题，引出Transformer模型的优势。
- 注意力机制
   - 介绍注意力机制的基本原理和作用。
   - 解释自注意力机制和多头注意力机制的概念。
   - 强调注意力机制在Transformer模型中的关键作用。

- Transformer模型的结构
   - 描述Transformer模型的整体结构，包括编码器和解码器。
   - 详细介绍编码器和解码器的构成，包括多层自注意力和全连接层。
   - 解释输入和输出的数据流动方式。

- 自注意力机制
   - 阐述自注意力机制的工作原理，包括注意力权重的计算和上下文向量的生成。
   - 说明自注意力机制相比于传统序列模型的优势，如捕捉长距离依赖关系等。

- 多头注意力机制
   - 介绍多头注意力机制的概念和作用。
   - 解释多头注意力机制是如何并行计算多个注意力权重和上下文向量的。

- 位置编码
   - 说明位置编码的作用和需要引入的原因。
   - 描述位置编码的实现方式，如使用正弦和余弦函数进行编码。

- 模型训练和推理
   - 解释Transformer模型的训练阶段和推理阶段的区别。
   - 提及训练时使用的损失函数、优化算法以及学习率调度方法。

- Transformer模型的改进和扩展
   - 介绍一些Transformer模型的改进和扩展方法，如BERT、GPT等。
   - 强调这些改进和扩展模型在不同任务上的性能提升和应用价值。

### 👉[Tokenizer](./zh/token.md)
在NLP任务中，计算机能够理解和处理的最小单位便是Token。本节将从几个不同类别的分词器入手介绍主要的内容，可以看到根据不同的分词器,有着不同的分词效果。
以下是Tokenizer这个章节的框架结构：

- 介绍Tokenizer的背景和作用，以及它在自然语言处理中的重要性。
- 强调Tokenizer在文本预处理中的关键作用，包括分词、标记化、词形还原等。

- 基本概念
   - 定义Tokenizer的基本概念，包括Token、词汇表、序列等。
   - 解释Tokenization的概念和过程，即将文本划分成一系列不可再分的单元。

- 常见Tokenizer算法和方法
   - 介绍基于规则的Tokenizer算法，如基于空格、标点符号的简单分词方法。
   - 描述基于机器学习的Tokenizer方法，如统计分词、最大匹配法等。
   - 提及基于深度学习的Tokenizer模型，如基于循环神经网络或Transformer的分词模型。

- 分词规范和语言相关性
   - 说明分词规范的重要性，如中文的《现代汉语词典》和英文的格林斯潘规范。
   - 强调不同语言的分词特点和挑战，例如中文和英文的分词方法差异。

- 词形还原和词性标注
   - 介绍词形还原的作用和方法，即将不同形态的单词还原为原形。
   - 简要解释词性标注的概念和作用，即为每个单词标注其词性。

- 常见Tokenizer库和工具
   - 介绍一些常见的开源Tokenizer库和工具，如NLTK、spaCy、BERT Tokenizer等。
   - 强调这些工具在不同任务和语言处理方面的应用场景和优势。

### 👉[BERT]()
以下是BERT（Bidirectional Encoder Representations from Transformers）模型的框架结构概述：

- 介绍BERT模型的背景和意义，以及它在自然语言处理领域的重要性。
- 强调BERT模型在语言理解任务中的卓越表现和广泛应用。

- BERT模型的结构
   - 描述BERT模型的整体结构，包括编码器和预训练的目标任务。
   - 详细介绍BERT模型的编码器部分，即多层Transformer编码器的堆叠。
   - 解释编码器的输入是经过WordPiece或其他分词方法处理过的文本序列。

- BERT模型的预训练
   - 解释BERT模型的预训练阶段，即在大规模未标注文本上进行的自监督学习。
   - 强调预训练目标任务，如Masked Language Model（MLM）和Next Sentence Prediction（NSP）。
   - 提及预训练所需的大规模语料库和训练策略，如分批次训练和动态掩码。

- BERT模型的微调
   - 解释BERT模型的微调阶段，即在特定任务上针对标注数据进行的有监督学习。
   - 提及微调的常见任务，如文本分类、命名实体识别和问答等。
   - 强调微调阶段的模型架构调整和任务特定的输出层。

### 👉[GPT Series](./zh/gpt2_finetuning.md)
以下是GPT-2（Generative Pre-trained Transformer 2）模型的概述框架：

- 介绍GPT-2模型的背景和作用，以及它在自然语言处理领域中的重要性。
- 强调GPT-2模型在语言生成任务中的卓越性能和广泛应用。
- GPT-2模型的结构
   - 描述GPT-2模型的整体结构，包括多层Transformer解码器的堆叠。
   - 解释GPT-2模型是一个单向语言模型，通过自回归生成文本序列。

- GPT-2模型的训练过程
   - 详细介绍GPT-2模型的训练过程，包括预训练和微调两个阶段。
   - 解释预训练阶段使用的目标任务，如掩码语言模型和下一句预测。
   - 强调GPT-2模型采用无监督学习，在大规模无标注文本上进行自学习。

- GPT-2模型的生成能力
   - 阐述GPT-2模型通过预训练学习到的语言表示能力，能够生成连贯、有上下文的文本序列。
   - 提及GPT-2模型在文本生成任务上的出色表现和广泛应用，如对话生成、文章创作等。

- GPT-2模型的应用领域
   - 介绍GPT-2模型在自然语言处理任务中的应用，如机器翻译、文本摘要和问答系统等。
   - 强调GPT-2模型在生成长文本和处理复杂语境方面的优势。

### 👉[T5 Series]()
本节从以下几个方面介绍T5模型：
- T5模型概述
- T5模型与BERT的区别
- T5模型的架构
- T5模型训练过程
- T5模型应用场景
- T5模型的优势和不足
- T5模型的未来发展方向